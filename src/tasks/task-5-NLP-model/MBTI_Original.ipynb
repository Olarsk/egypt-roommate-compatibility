{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a338d628",
      "metadata": {
        "id": "a338d628"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d52ff49",
      "metadata": {
        "id": "3d52ff49"
      },
      "outputs": [],
      "source": [
        "import nltk  \n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42146589",
      "metadata": {
        "id": "42146589"
      },
      "outputs": [],
      "source": [
        "mbti_data = pd.read_csv('mbti_1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "715adc65",
      "metadata": {
        "id": "715adc65"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk import ne_chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c094b2e3",
      "metadata": {
        "id": "c094b2e3"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Stop Word Removal\n",
        "def remove_stop_words(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "def perform_ner(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    ner_tags = []\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        pos_tags = pos_tag(tokens)\n",
        "        ner_tags.extend(ne_chunk(pos_tags))\n",
        "    return ner_tags\n",
        "\n",
        "# Part-of-Speech (POS) Tagging\n",
        "def perform_pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    return pos_tags\n",
        "\n",
        "# Sentiment Analysis\n",
        "def perform_sentiment_analysis(text):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment_score = sia.polarity_scores(text)\n",
        "    return sentiment_score\n",
        "\n",
        "# Apply NLP techniques to the 'posts' column\n",
        "mbti_data['tokens'] = mbti_data['posts'].apply(tokenize_text)\n",
        "mbti_data['filtered_tokens'] = mbti_data['tokens'].apply(remove_stop_words)\n",
        "mbti_data['ner_tags'] = mbti_data['posts'].apply(perform_ner)\n",
        "mbti_data['pos_tags'] = mbti_data['posts'].apply(perform_pos_tagging)\n",
        "mbti_data['sentiment_scores'] = mbti_data['posts'].apply(perform_sentiment_analysis)\n",
        "\n",
        "# Print the processed data\n",
        "for index, row in mbti_data.iterrows():\n",
        "    print(\"Post:\", row['posts'])\n",
        "    print(\"Tokens:\", row['tokens'])\n",
        "    print(\"Filtered Tokens:\", row['filtered_tokens'])\n",
        "    print(\"NER Tags:\", row['ner_tags'])\n",
        "    print(\"POS Tags:\", row['pos_tags'])\n",
        "    print(\"Sentiment Scores:\", row['sentiment_scores'])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34debb86",
      "metadata": {
        "id": "34debb86"
      },
      "outputs": [],
      "source": [
        "mbti_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5e29b6",
      "metadata": {
        "id": "4e5e29b6"
      },
      "outputs": [],
      "source": [
        "pip install clean-text[sklearn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0daf691e",
      "metadata": {
        "id": "0daf691e"
      },
      "outputs": [],
      "source": [
        "from cleantext.sklearn import CleanTransformer\n",
        "\n",
        "cleaner = CleanTransformer(no_punct=True, lower=True,no_urls=True)\n",
        "\n",
        "cleaner.transform(mbti_data['posts'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "055f8ed4",
      "metadata": {
        "id": "055f8ed4"
      },
      "outputs": [],
      "source": [
        "def remove_url(a):\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbcfbb5f",
      "metadata": {
        "id": "dbcfbb5f"
      },
      "outputs": [],
      "source": [
        "from cleantext import clean\n",
        "\n",
        "clean(mbti_data[],\n",
        "    fix_unicode=True,               # fix various unicode errors\n",
        "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "    lower=True,                     # lowercase text\n",
        "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
        "    no_urls=False,                  # replace all URLs with a special token\n",
        "    no_emails=False,                # replace all email addresses with a special token\n",
        "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
        "    no_numbers=False,               # replace all numbers with a special token\n",
        "    no_digits=False,                # replace all digits with a special token\n",
        "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
        "    no_punct=False,                 # remove punctuations\n",
        "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "    replace_with_url=\"youtube.com\",\n",
        "    replace_with_email=\"<EMAIL>\",\n",
        "    replace_with_phone_number=\"<PHONE>\",\n",
        "    replace_with_number=\"<NUMBER>\",\n",
        "    replace_with_digit=\"0\",\n",
        "    replace_with_currency_symbol=\"<CUR>\",\n",
        "    lang=\"en\"                       # set to 'de' for German special handling\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4860279a",
      "metadata": {
        "id": "4860279a"
      },
      "outputs": [],
      "source": [
        "pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9000e96b",
      "metadata": {
        "id": "9000e96b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}